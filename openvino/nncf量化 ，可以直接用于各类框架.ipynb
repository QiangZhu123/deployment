{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "000513af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, tensorflow, onnx, openvino\n"
     ]
    }
   ],
   "source": [
    "import nncf\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557d7b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nncf\n",
    "import torch\n",
    "\n",
    "calibration_loader = torch.utils.data.DataLoader(...)\n",
    "\n",
    "def transform_fn(data_item):\n",
    "    images, _ = data_item\n",
    "    #return images  torch\n",
    "    return images.numpy()\n",
    "    \n",
    "    \n",
    "calibration_dataset = nncf.Dataset(calibration_loader, transform_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8800c7ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mresnet50(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#这个还是需要转化\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#quantized_model = nncf.quantize(model, calibration_dataset)\u001b[39;00m\n",
      "File \u001b[1;32mD:\\python\\lib\\site-packages\\torchvision\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodulefinder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Module\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets, io, models, ops, transforms, utils\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\python\\lib\\site-packages\\torchvision\\models\\__init__.py:9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmnasnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmobilenet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshufflenetv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:975\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1074\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "#这个还是需要转化\n",
    "#quantized_model = nncf.quantize(model, calibration_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70234df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model to transform quantized operations to int8\n",
    "model_int8 = ov.compile_model(quantized_model)\n",
    "\n",
    "input_fp32 = ... # FP32 model input\n",
    "res = model_int8(input_fp32)\n",
    "\n",
    "# save the model\n",
    "ov.save_model(quantized_model, \"quantized_model.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec2eca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.runtime import Core\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import openvino as ov\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve(\n",
    "    url='https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/main/notebooks/utils/notebook_utils.py',\n",
    "    filename='notebook_utils.py'\n",
    ")\n",
    "\n",
    "from notebook_utils import download_file\n",
    "\n",
    "import numpy as np\n",
    "import openvino as ov\n",
    "import torch\n",
    "import IPython.display as ipd\n",
    "\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Set the data and model directories, model source URL and model filename.\n",
    "MODEL_DIR = Path(\"model\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a77bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\", ctc_loss_reduction=\"mean\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa78a3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "MAX_SEQ_LENGTH = 30480\n",
    "ov_model = ov.convert_model(torch_model, example_input=torch.zeros([1, MAX_SEQ_LENGTH], dtype=torch.float))\n",
    "\n",
    "ir_model_path = MODEL_DIR / \"wav2vec2_base.xml\"\n",
    "ov.save_model(ov_model, ir_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b85812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "test_sample = dataset[0][\"audio\"]\n",
    "\n",
    "\n",
    "# define preprocessing function for converting audio to input values for model\n",
    "def map_to_input(batch):\n",
    "    preprocessed_signal = processor(batch[\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\", sampling_rate=batch['audio']['sampling_rate'])\n",
    "    input_values = preprocessed_signal.input_values\n",
    "    batch['input_values'] = input_values\n",
    "    return batch\n",
    "\n",
    "\n",
    "# apply preprocessing function to dataset and remove audio column, to save memory as we do not need it anymore\n",
    "dataset = dataset.map(map_to_input, batched=False, remove_columns=[\"audio\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1334d3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nncf\n",
    "from nncf.parameters import ModelType\n",
    "\n",
    "def transform_fn(data_item):\n",
    "\n",
    "    return np.array(data_item[\"input_values\"])\n",
    "\n",
    "\n",
    "calibration_dataset = nncf.Dataset(dataset, transform_fn)\n",
    "\n",
    "quantized_model = nncf.quantize(\n",
    "    ov_model,\n",
    "    calibration_dataset,\n",
    "    model_type=ModelType.TRANSFORMER,  # specify additional transformer patterns in the model\n",
    "    ignored_scope=nncf.IgnoredScope(\n",
    "        names=[\n",
    "            \"__module.wav2vec2.feature_extractor.conv_layers.1.conv/aten::_convolution/Convolution\",\n",
    "            \"__module.wav2vec2.feature_extractor.conv_layers.2.conv/aten::_convolution/Convolution\",\n",
    "            \"__module.wav2vec2.feature_extractor.conv_layers.3.conv/aten::_convolution/Convolution\",\n",
    "            \"__module.wav2vec2.feature_extractor.conv_layers.0.conv/aten::_convolution/Convolution\",\n",
    "        ],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dca59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "core=ov.Core()\n",
    "compiled_model = core.compile_model(model=quantized_model, device_name='CPU')\n",
    "\n",
    "input_data = np.expand_dims(test_sample[\"array\"], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f61ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = compiled_model(input_data)[0]\n",
    "predicted_ids = np.argmax(predictions, axis=-1)\n",
    "transcription = processor.batch_decode(torch.from_numpy(predicted_ids))\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb939ab",
   "metadata": {},
   "source": [
    "# Weight Compression\n",
    "    只针对模型的权重，\n",
    "    enables inference of exceptionally large models that cannot be accommodated in the device memory;\n",
    "\n",
    "    reduces storage and memory overhead, making models more lightweight and less resource intensive for deployment;\n",
    "\n",
    "    improves inference speed by reducing the latency of memory access when computing the operations with weights, for example, Linear layers. The weights are smaller and thus faster to load from memory;\n",
    "\n",
    "    unlike quantization, does not require sample data to calibrate the range of activation values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4f6303",
   "metadata": {},
   "source": [
    "#### 8-bit weight quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a9e57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nncf import compress_weights\n",
    "\n",
    "model = compress_weights(model) # model is openvino.Model object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140049fe",
   "metadata": {},
   "source": [
    "##### 4-bit weight quantization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0665f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nncf import compress_weights, CompressWeightsMode\n",
    "\n",
    "# Example: Compressing weights with INT4_ASYM mode, group size of 64, and 90% INT4 ratio\n",
    "compressed_model = compress_weights(\n",
    "  model,\n",
    "  mode=CompressWeightsMode.INT4_ASYM,\n",
    "  group_size=64,\n",
    "  ratio=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f26be1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e75c96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0732237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a697c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
