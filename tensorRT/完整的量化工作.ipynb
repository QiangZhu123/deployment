{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9781df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models, datasets\n",
    "\n",
    "import pytorch_quantization\n",
    "from pytorch_quantization import nn as quant_nn\n",
    "from pytorch_quantization import quant_modules\n",
    "from pytorch_quantization import calib\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(pytorch_quantization.__version__)\n",
    "\n",
    "import os\n",
    "import tensorrt as trt\n",
    "import numpy as np\n",
    "import time\n",
    "import wget\n",
    "import tarfile\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8742f1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#下载数据\n",
    "def download_data(DATA_DIR):\n",
    "    if os.path.exists(DATA_DIR):\n",
    "        if not os.path.exists(os.path.join(DATA_DIR, 'imagenette2-320')):\n",
    "            url = 'https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-320.tgz'\n",
    "            wget.download(url)\n",
    "            # open file\n",
    "            file = tarfile.open('imagenette2-320.tgz')\n",
    "            # extracting file\n",
    "            file.extractall(DATA_DIR)\n",
    "            file.close()\n",
    "    else:\n",
    "        print(\"This directory doesn't exist. Create the directory and run again\")\n",
    "        \n",
    "if not os.path.exists(\"./data\"):\n",
    "    os.mkdir(\"./data\")\n",
    "download_data(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda85243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define main data directory\n",
    "DATA_DIR = './data/imagenette2-320' \n",
    "# Define training and validation data paths\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train') \n",
    "VAL_DIR = os.path.join(DATA_DIR, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f336c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing Transformations on the dataset and defining training and validation dataloaders\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            ])\n",
    "train_dataset = datasets.ImageFolder(TRAIN_DIR, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(VAL_DIR, transform=transform)\n",
    "calib_dataset = torch.utils.data.random_split(val_dataset, [2901, 1024])[1]\n",
    "\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "val_dataloader = data.DataLoader(val_dataset, batch_size=64, shuffle=False, drop_last=True)\n",
    "calib_dataloader = data.DataLoader(calib_dataset, batch_size=64, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4186380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising an image from the validation set\n",
    "import matplotlib.pyplot as plt\n",
    "for images, labels in val_dataloader:\n",
    "    print(labels[0])\n",
    "    image = images[0]\n",
    "    img = image.swapaxes(0, 1)\n",
    "    img = img.swapaxes(1, 2)\n",
    "    plt.imshow(img)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42be8d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function allows you to set the all the parameters to not have gradients, \n",
    "# allowing you to freeze the model and not undergo training during the train step. \n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "feature_extract = True #This varaible can be set False if you want to finetune the model by updating all the parameters. \n",
    "model = models.mobilenet_v2(pretrained=True)\n",
    "set_parameter_requires_grad(model, feature_extract)\n",
    "#Define a classification head for 10 classes.\n",
    "model.classifier[1] = nn.Linear(1280, 10)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93394b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare Learning rate\n",
    "lr = 0.0001\n",
    "\n",
    "# Use cross entropy loss for classification and SGD optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4538a502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for training, evalution, saving checkpoint and train parameter setting function\n",
    "def train(model, dataloader, crit, opt, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch, (data, labels) in enumerate(dataloader):\n",
    "        data, labels = data.cuda(), labels.cuda(non_blocking=True)\n",
    "        opt.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = crit(out, labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        running_loss += loss.item()\n",
    "        if batch % 100 == 99:\n",
    "            print(\"Batch: [%5d | %5d] loss: %.3f\" % (batch + 1, len(dataloader), running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "        \n",
    "def evaluate(model, dataloader, crit, epoch):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loss = 0.0\n",
    "    class_probs = []\n",
    "    class_preds = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, labels in dataloader:\n",
    "            data, labels = data.cuda(), labels.cuda(non_blocking=True)\n",
    "            out = model(data)\n",
    "            loss += crit(out, labels)\n",
    "            preds = torch.max(out, 1)[1]\n",
    "            class_preds.append(preds)\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "def save_checkpoint(state, ckpt_path=\"checkpoint.pth\"):\n",
    "    torch.save(state, ckpt_path)\n",
    "    print(\"Checkpoint saved\")\n",
    "    \n",
    "# Helper function to benchmark the model\n",
    "cudnn.benchmark = True\n",
    "def benchmark(model, input_shape=(1024, 1, 32, 32), dtype='fp32', nwarmup=50, nruns=1000):\n",
    "    input_data = torch.randn(input_shape)\n",
    "    input_data = input_data.to(\"cuda\")\n",
    "    if dtype=='fp16':\n",
    "        input_data = input_data.half()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for _ in range(nwarmup):\n",
    "            features = model(input_data)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    timings = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(1, nruns+1):\n",
    "            start_time = time.time()\n",
    "            output = model(input_data)\n",
    "            torch.cuda.synchronize()\n",
    "            end_time = time.time()\n",
    "            timings.append(end_time - start_time)\n",
    "\n",
    "    print('Average batch time: %.2f ms'%(np.mean(timings)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c41d028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for 5 epochs to attain an acceptable accuracy.\n",
    "num_epochs=5\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch: [%5d / %5d] LR: %f' % (epoch + 1, num_epochs, lr))\n",
    "\n",
    "    train(model, train_dataloader, criterion, optimizer, epoch)\n",
    "    test_acc = evaluate(model, val_dataloader, criterion, epoch)\n",
    "\n",
    "    print(\"Test Acc: {:.2f}%\".format(100 * test_acc))\n",
    "    \n",
    "save_checkpoint({'epoch': epoch + 1,\n",
    "                 'model_state_dict': model.state_dict(),\n",
    "                 'acc': test_acc,\n",
    "                 'opt_state_dict': optimizer.state_dict()\n",
    "                },\n",
    "                ckpt_path=\"models/mobilenetv2_base_ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08acf7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the baseline model\n",
    "test_acc = evaluate(model, val_dataloader, criterion, 0)\n",
    "print(\"Mobilenetv2 Baseline accuracy: {:.2f}%\".format(100 * test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523b15e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting to Onnx\n",
    "dummy_input = torch.randn(64, 3, 224, 224, device='cuda')\n",
    "input_names = [ \"actual_input_1\" ]\n",
    "output_names = [ \"output1\" ]\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"models/mobilenetv2_base.onnx\",\n",
    "    verbose=False,\n",
    "    opset_version=13,\n",
    "    do_constant_folding = False)\n",
    "\n",
    "# Converting ONNX model to TRT\n",
    "!trtexec --onnx=models/mobilenetv2_base.onnx --saveEngine=models/mobilenetv2_base.trt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124a5e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_modules.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e299bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define Mobilenetv2 again just like we did above\n",
    "# All the regular conv, FC layers will be converted to their quantized counterparts due to quant_modules.initialize()\n",
    "feature_extract = True\n",
    "q_model = models.mobilenet_v2(pretrained=True)\n",
    "set_parameter_requires_grad(q_model, feature_extract)\n",
    "q_model.classifier[1] = nn.Linear(1280, 10)\n",
    "q_model = q_model.cuda()\n",
    "\n",
    "# mobilenetv2_base_ckpt is the checkpoint generated from Step 2 : Training a baseline Mobilenetv2 model.\n",
    "ckpt = torch.load(\"./models/mobilenetv2_base_ckpt\")\n",
    "modified_state_dict={}\n",
    "for key, val in ckpt[\"model_state_dict\"].items():\n",
    "    # Remove 'module.' from the key names\n",
    "    if key.startswith('module'):\n",
    "        modified_state_dict[key[7:]] = val\n",
    "    else:\n",
    "        modified_state_dict[key] = val\n",
    "\n",
    "# Load the pre-trained checkpoint\n",
    "q_model.load_state_dict(modified_state_dict)\n",
    "optimizer.load_state_dict(ckpt[\"opt_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a822823c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_amax(model, **kwargs):\n",
    "    # Load calib result\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                if isinstance(module._calibrator, calib.MaxCalibrator):\n",
    "                    module.load_calib_amax()\n",
    "                else:\n",
    "                    module.load_calib_amax(**kwargs)\n",
    "    model.cuda()\n",
    "\n",
    "def collect_stats(model, data_loader, num_batches):\n",
    "    \"\"\"Feed data to the network and collect statistics\"\"\"\n",
    "    # Enable calibrators\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                module.disable_quant()\n",
    "                module.enable_calib()\n",
    "            else:\n",
    "                module.disable()\n",
    "\n",
    "    # Feed data to the network for collecting stats\n",
    "    for i, (image, _) in tqdm(enumerate(data_loader), total=num_batches):\n",
    "        model(image.cuda())\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "\n",
    "    # Disable calibrators\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                module.enable_quant()\n",
    "                module.disable_calib()\n",
    "            else:\n",
    "                module.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c45c2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate the model using max calibration technique.\n",
    "with torch.no_grad():\n",
    "    collect_stats(q_model, train_dataloader, num_batches=16)\n",
    "    compute_amax(q_model, method=\"max\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0870b3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the PTQ model\n",
    "torch.save(q_model.state_dict(), \"./models/mobilenetv2_ptq.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e88fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the PTQ Model \n",
    "test_acc = evaluate(q_model, val_dataloader, criterion, 0)\n",
    "print(\"Mobilenetv2 PTQ accuracy: {:.2f}%\".format(100 * test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d5176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set static member of TensorQuantizer to use Pytorch’s own fake quantization functions\n",
    "quant_nn.TensorQuantizer.use_fb_fake_quant = True\n",
    "\n",
    "# Exporting to ONNX\n",
    "dummy_input = torch.randn(64, 3, 224, 224, device='cuda')\n",
    "input_names = [ \"actual_input_1\" ]\n",
    "output_names = [ \"output1\" ]\n",
    "torch.onnx.export(\n",
    "    q_model,\n",
    "    dummy_input,\n",
    "    \"models/mobilenetv2_ptq.onnx\",\n",
    "    verbose=False,\n",
    "    opset_version=13,\n",
    "    do_constant_folding = False)\n",
    "\n",
    "# Converting ONNX model to TRT\n",
    "!trtexec --onnx=models/mobilenetv2_ptq.onnx --int8 --saveEngine=models/mobilenetv2_ptq.trt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1540364",
   "metadata": {},
   "source": [
    "# QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c22a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune the QAT model for 2 epochs\n",
    "num_epochs=2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch: [%5d / %5d] LR: %f' % (epoch + 1, num_epochs, lr))\n",
    "\n",
    "    train(q_model, train_dataloader, criterion, optimizer, epoch)\n",
    "    test_acc = evaluate(q_model, val_dataloader, criterion, epoch)\n",
    "\n",
    "    print(\"Test Acc: {:.2f}%\".format(100 * test_acc))\n",
    "    \n",
    "save_checkpoint({'epoch': epoch + 1,\n",
    "                 'model_state_dict': q_model.state_dict(),\n",
    "                 'acc': test_acc,\n",
    "                 'opt_state_dict': optimizer.state_dict()\n",
    "                },\n",
    "                ckpt_path=\"models/mobilenetv2_qat_ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c47310c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate the QAT model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m test_acc \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m(q_model, val_dataloader, criterion, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMobilenetv2 QAT accuracy: \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m test_acc))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'evaluate' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate the QAT model\n",
    "test_acc = evaluate(q_model, val_dataloader, criterion, 0)\n",
    "print(\"Mobilenetv2 QAT accuracy: {:.2f}%\".format(100 * test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666b3caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set static member of TensorQuantizer to use Pytorch’s own fake quantization functions\n",
    "quant_nn.TensorQuantizer.use_fb_fake_quant = True\n",
    "\n",
    "# Exporting to ONNX\n",
    "dummy_input = torch.randn(64, 3, 224, 224, device='cuda')\n",
    "input_names = [ \"actual_input_1\" ]\n",
    "output_names = [ \"output1\" ]\n",
    "torch.onnx.export(\n",
    "    q_model,\n",
    "    dummy_input,\n",
    "    \"models/mobilenetv2_qat.onnx\",\n",
    "    verbose=False,\n",
    "    opset_version=13,\n",
    "    do_constant_folding = False)\n",
    "\n",
    "# Converting ONNX model to TRT\n",
    "!trtexec --onnx=models/mobilenetv2_qat.onnx --int8 --saveEngine=models/mobilenetv2_qat.trt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b22294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed libraries and define the evaluate function\n",
    "\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import time \n",
    "\n",
    "def evaluate_trt(engine_path, dataloader, batch_size):\n",
    "    \n",
    "    def predict(batch): # result gets copied into output\n",
    "        # transfer input data to device\n",
    "        cuda.memcpy_htod_async(d_input, batch, stream)\n",
    "        # execute model\n",
    "        context.execute_async_v2(bindings, stream.handle, None)\n",
    "        # transfer predictions back\n",
    "        cuda.memcpy_dtoh_async(output, d_output, stream)\n",
    "        # syncronize threads\n",
    "        stream.synchronize()\n",
    "        return output\n",
    "    \n",
    "    with open(engine_path, 'rb') as f, trt.Runtime(trt.Logger(trt.Logger.WARNING)) as runtime, runtime.deserialize_cuda_engine(f.read()) as engine, engine.create_execution_context() as context:\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for images, labels in val_dataloader:\n",
    "            input_batch = images.numpy()\n",
    "            labels = labels.numpy()\n",
    "            output = np.empty([batch_size, 10], dtype = np.float32) \n",
    "\n",
    "            # Now allocate input and output memory, give TRT pointers (bindings) to it:\n",
    "            d_input = cuda.mem_alloc(1 * input_batch.nbytes)\n",
    "            d_output = cuda.mem_alloc(1 * output.nbytes)\n",
    "            bindings = [int(d_input), int(d_output)]\n",
    "\n",
    "            stream = cuda.Stream()\n",
    "            preds = predict(input_batch)\n",
    "            pred_labels = []\n",
    "            for pred in preds:\n",
    "                pred_label = (-pred).argsort()[0]\n",
    "                pred_labels.append(pred_label)\n",
    "\n",
    "            total += len(labels)\n",
    "            correct += (pred_labels == labels).sum()\n",
    "    \n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec886eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and benchmark the performance of the baseline TRT model (TRT FP32 Model)\n",
    "batch_size = 64\n",
    "test_acc = evaluate_trt(\"models/mobilenetv2_base.trt\", val_dataloader, batch_size)\n",
    "print(\"Mobilenetv2 TRT Baseline accuracy: {:.2f}%\".format(100 * test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb971837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the PTQ model\n",
    "batch_size = 64\n",
    "test_acc = evaluate_trt(\"models/mobilenetv2_ptq.trt\", val_dataloader, batch_size)\n",
    "print(\"Mobilenetv2 TRT PTQ accuracy: {:.2f}%\".format(100 * test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c5b311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the QAT model\n",
    "batch_size = 64\n",
    "test_acc = evaluate_trt(\"models/mobilenetv2_qat.trt\", val_dataloader, batch_size)\n",
    "print(\"Mobilenetv2 TRT PTQ accuracy: {:.2f}%\".format(100 * test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350c0c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccc1f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a8f697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7428b9fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060ca00e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d3a475",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
