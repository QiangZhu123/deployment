{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dd7e73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import onnx\n",
    "#!pip install onnxruntime==1.8.1\n",
    "import onnxruntime\n",
    "from torch.autograd import Function\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbf6f327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['ONNXRUNTIME_DIR']='D:\\桌面\\包\\onnxruntime-linux-x64-1.8\\onnxruntime-linux-x64-1.8.1'\n",
    "os.environ['LD_LIBRARY_PATH']='D:\\桌面\\包\\onnxruntime-linux-x64-1.8\\onnxruntime-linux-x64-1.8.1\\lib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caef3f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/microsoft/onnxruntime/releases/download/v1.8.1/onnxruntime-linux-x64-1.8.1.tgz\n",
    "!tar -zxvf onnxruntime-linux-x64-1.8.1.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d878522e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\myjupyter\\深度学习\\deployment\\onnx\\plugin\n",
      "D:\\myjupyter\\深度学习\\deployment\\onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n"
     ]
    }
   ],
   "source": [
    "!mkdir plugin\n",
    "%cd plugin\n",
    "!mkdir cpu\n",
    "%cd ..\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fbc8093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\myjupyter\\深度学习\\deployment\\onnx\\plugin\\cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n"
     ]
    }
   ],
   "source": [
    "%cd plugin/cpu\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bad87e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_file =\"\"\"\n",
    "#include \"onnxruntime_register.h\"\n",
    "#include \"custom.h\"\n",
    "#include \"ort_mmcv_utils.h\"\n",
    "\n",
    "const char *c_MMCVOpDomain = \"mmcv\";\n",
    "CustomOp  c_CustomOp;\n",
    "\n",
    "OrtStatus *ORT_API_CALL RegisterCustomOps(OrtSessionOptions *options,\n",
    "                                          const OrtApiBase *api) {\n",
    "  OrtCustomOpDomain *domain = nullptr;\n",
    "  const OrtApi *ortApi = api->GetApi(ORT_API_VERSION);\n",
    "\n",
    "  if (auto status = ortApi->CreateCustomOpDomain(c_MMCVOpDomain, &domain)) {\n",
    "    return status;\n",
    "  }\n",
    "\n",
    "\n",
    "  if (auto status = ortApi->CustomOpDomain_Add(domain, &c_CustomOp)) {\n",
    "    return status;\n",
    "  }\n",
    "\n",
    "  return ortApi->AddCustomOpDomain(options, domain);\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "with open('onnxruntime_register.cpp','w') as f:\n",
    "    f.write(head_file)\n",
    "head_file =\"\"\"\n",
    "#include \"custom.h\"\n",
    "#include <assert.h>\n",
    "#include <algorithm>\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "\n",
    "#include \"../ort_mmcv_utils.h\"\n",
    "\n",
    "void CustomKernel::Compute(OrtKernelContext *context) {\n",
    "    const float value = float(value_);\n",
    "    const OrtValue *inputs = ort_.KernelContext_GetInput(context, 0);\n",
    "    const float * inputs_data = reinterpret_cast<const float*>(ort_.GetTensorData<const float*>(inputs));\n",
    "    \n",
    "    OrtTensorDimensions input_dims(ort_, inputs); \n",
    "    std::cout<<input_dims.data()<<std::endl;\n",
    "    int nums=1;\n",
    "    int size = int(input_dims.size());\n",
    "    for (int i =0;i<size;i++)\n",
    "        nums *=input_dims.data()[i]; \n",
    "    \n",
    "    OrtValue* output = ort_.KernelContext_GetOutput(context, 0, input_dims.data(), input_dims.size());\n",
    "    float* out = ort_.GetTensorMutableData<float>(output);\n",
    "    for (int index =0;index<nums;index++)\n",
    "        out[index]=inputs_data[index] + value;\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "with open('custom.cpp','w') as f:\n",
    "    f.write(head_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b411fe78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\myjupyter\\深度学习\\deployment\\onnx\\plugin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cf80222",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_file = \"\"\"\n",
    "#ifndef ORT_MMCV_UTILS_H\n",
    "#define ORT_MMCV_UTILS_H\n",
    "#include <onnxruntime_cxx_api.h>\n",
    "\n",
    "#include <vector>\n",
    "\n",
    "struct OrtTensorDimensions : std::vector<int64_t> {\n",
    "  OrtTensorDimensions(Ort::CustomOpApi ort, const OrtValue* value) {\n",
    "    OrtTensorTypeAndShapeInfo* info = ort.GetTensorTypeAndShape(value);\n",
    "    std::vector<int64_t>::operator=(ort.GetTensorShape(info));\n",
    "    ort.ReleaseTensorTypeAndShapeInfo(info);\n",
    "  }\n",
    "};\n",
    "#endif  // ORT_MMCV_UTILS_H\n",
    "\n",
    "\"\"\"\n",
    "with open('ort_mmcv_utils.h','w') as f:\n",
    "    f.write(head_file)\n",
    "    \n",
    "head_file = \"\"\"\n",
    "#ifndef ONNXRUNTIME_REGISTER_H\n",
    "#define ONNXRUNTIME_REGISTER_H\n",
    "#include <onnxruntime_c_api.h>\n",
    "\n",
    "#ifdef __cplusplus\n",
    "extern \"C\" {\n",
    "#endif\n",
    "\n",
    "OrtStatus *ORT_API_CALL RegisterCustomOps(OrtSessionOptions *options,\n",
    "                                          const OrtApiBase *api);\n",
    "\n",
    "#ifdef __cplusplus\n",
    "}\n",
    "#endif\n",
    "#endif  // ONNXRUNTIME_REGISTER_H\n",
    "\"\"\"\n",
    "with open('onnxruntime_register.h','w') as f:\n",
    "    f.write(head_file)\n",
    "head_file = \"\"\"\n",
    "#ifndef ONNXRUNTIME_CUSTOM_H\n",
    "#define ONNXRUNTIME_CUSTOM_H\n",
    "\n",
    "#include <onnxruntime_cxx_api.h>\n",
    "\n",
    "\n",
    "struct CustomKernel {\n",
    "  CustomKernel(OrtApi api, const OrtKernelInfo *info) : api_(api), ort_(api_), info_(info)\n",
    "  {\n",
    "    value_ = ort_.KernelInfoGetAttribute<float>(info, \"value_add\");\n",
    "\n",
    "  // create allocator\n",
    "  // allocator_ = Ort::AllocatorWithDefaultOptions();\n",
    "  };\n",
    "\n",
    "  void Compute(OrtKernelContext *context);\n",
    "\n",
    " protected:\n",
    "  OrtApi api_;\n",
    "  Ort::CustomOpApi ort_;\n",
    "  const OrtKernelInfo *info_;\n",
    "  // Ort::AllocatorWithDefaultOptions allocator_;\n",
    "private:\n",
    "  float value_;\n",
    "\n",
    "};\n",
    "\n",
    "struct CustomOp : Ort::CustomOpBase<CustomOp, CustomKernel> {\n",
    "  void *CreateKernel(OrtApi api, const OrtKernelInfo *info) const {\n",
    "    return new CustomKernel(api, info);\n",
    "  };\n",
    "\n",
    "  const char *GetName() const { return \"Custom\"; };\n",
    "\n",
    "  size_t GetInputTypeCount() const { return 1; };\n",
    "  ONNXTensorElementDataType GetInputType(size_t /*index*/) const {\n",
    "    return ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT;\n",
    "  };\n",
    "\n",
    "  size_t GetOutputTypeCount() const { return 1; };\n",
    "  ONNXTensorElementDataType GetOutputType(size_t index) const {\n",
    "    return ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT;\n",
    "  }\n",
    "\n",
    "  // force cpu\n",
    "  const char *GetExecutionProviderType() const {\n",
    "    return \"CPUExecutionProvider\";\n",
    "  }\n",
    "};\n",
    "\n",
    "#endif\n",
    "\n",
    "\"\"\"\n",
    "with open('custom.h','w') as f:\n",
    "    f.write(head_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4775eef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\myjupyter\\深度学习\\deployment\\onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e4d52cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_file = \"\"\"\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from pkg_resources import DistributionNotFound, get_distribution, parse_version\n",
    "from setuptools import find_packages, setup\n",
    "\n",
    "from torch.utils.cpp_extension import BuildExtension\n",
    "EXT_TYPE = 'pytorch'\n",
    "cmd_class = {'build_ext': BuildExtension}\n",
    "\n",
    "def get_extensions():\n",
    "    import torch\n",
    "    extensions = []\n",
    "    ext_name = 'mmcv._ext_ort'\n",
    "    from torch.utils.cpp_extension import library_paths, include_paths\n",
    "    import onnxruntime\n",
    "    library_dirs = []\n",
    "    libraries = []\n",
    "    include_dirs = []\n",
    "    \n",
    "    ort_path = os.getenv('ONNXRUNTIME_DIR', '0')\n",
    "    library_dirs += [os.path.join(ort_path, 'lib')]\n",
    "    libraries.append('onnxruntime')\n",
    "    kwargs = {}\n",
    "    define_macros = []\n",
    "    extra_compile_args = {'cxx': []}\n",
    " \n",
    "    include_path = os.path.abspath('./plugin/')\n",
    "    include_dirs.append(include_path)\n",
    "    include_dirs.append(os.path.join(ort_path, 'include'))\n",
    "    \n",
    "\n",
    "    op_files = glob.glob('./plugin/cpu/*')\n",
    "\n",
    "    include_dirs += include_paths(cuda=False)\n",
    "    library_dirs += library_paths(cuda=False)\n",
    "\n",
    "    kwargs['library_dirs'] = library_dirs\n",
    "    kwargs['libraries'] = libraries\n",
    "\n",
    "    from setuptools import Extension\n",
    "    ext_ops = Extension(\n",
    "        name=ext_name,\n",
    "        sources=op_files,\n",
    "        include_dirs=include_dirs,\n",
    "        define_macros=define_macros,\n",
    "        extra_compile_args=extra_compile_args,\n",
    "        language='c++',\n",
    "        library_dirs=library_dirs,\n",
    "        libraries=libraries)\n",
    "    extensions.append(ext_ops)\n",
    "    return extensions\n",
    "setup(\n",
    "    name='mmcv',\n",
    "    ext_modules=get_extensions(),\n",
    "    cmdclass=cmd_class,)\n",
    "\n",
    "\"\"\"\n",
    "with open('setup.py','w') as f:\n",
    "    f.write(head_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6211084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396936db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.ops.load_library('/kaggle/working/onnxruntime-linux-x64-1.8.1/lib/libonnxruntime.so')\n",
    "torch.ops.load_library('/kaggle/working/onnxruntime-linux-x64-1.8.1/lib/libonnxruntime.so.1.8.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a437ffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def symbolic(g, inputs,value):\n",
    "        return g.op('mmcv::Custom',inputs,value_add_f=value)\n",
    "    @staticmethod\n",
    "    def forward(ctx,inputs,value):\n",
    "        outputs =inputs +value\n",
    "        return outputs\n",
    "testfunc = custom.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ba07d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "_value =3.0\n",
    "def func(inputs):\n",
    "    return testfunc(inputs,_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b55eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class WrapFunction(nn.Module):\n",
    "\n",
    "    def __init__(self, wrapped_function):\n",
    "        super(WrapFunction, self).__init__()\n",
    "        self.wrapped_function = wrapped_function\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.wrapped_function(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5268b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_custom_op_path='build/lib.linux-x86_64-3.7/mmcv/_ext_ort.cpython-37m-x86_64-linux-gnu.so'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6722d254",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_options = rt.SessionOptions()\n",
    "if ort_custom_op_path:\n",
    "    session_options.register_custom_ops_library(ort_custom_op_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98db92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_model = WrapFunction(func)\n",
    "wrapped_model.cpu().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd8135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(1,3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728dedd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.onnx.export(wrapped_model,(inputs,),'sample.onnx',input_names=['inputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc17cc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = onnx.load('sample.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a969ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = rt.InferenceSession('sample.onnx', session_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c8b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_output = sess.run(None, {\n",
    "    'inputs': inputs.detach().numpy(),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd9ded0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
